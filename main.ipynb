{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc4cd555",
   "metadata": {},
   "source": [
    "# NLP Text Classifier\n",
    "\n",
    "This project implements a text classification model using TensorFlow. The model preprocesses text data, removes stopwords, and trains a neural network to classify text into predefined categories.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6fef9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29c6c4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStoppingCallback(tf.keras.callbacks.Callback):\n",
    "    \"\"\"Stop training once accuracy reaches 90% on both training and validation sets, with a patience mechanism.\"\"\"\n",
    "    \n",
    "    def __init__(self, patience=2):\n",
    "        super(EarlyStoppingCallback, self).__init__()\n",
    "        self.patience = patience\n",
    "        self.best_val_accuracy = 0\n",
    "        self.wait = 0  \n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        current_val_accuracy = logs.get('val_accuracy')  \n",
    "\n",
    "        if current_val_accuracy is not None:\n",
    "            if current_val_accuracy > self.best_val_accuracy:\n",
    "                self.best_val_accuracy = current_val_accuracy \n",
    "                self.wait = 0 \n",
    "            else:\n",
    "                self.wait += 1  \n",
    "\n",
    "            if self.wait >= self.patience:\n",
    "                self.model.stop_training = True\n",
    "                print(f\"Early stopping: validation accuracy did not improve for {self.patience} epochs.\")\n",
    "\n",
    "        if logs.get('accuracy') >= 0.9 and current_val_accuracy >= 0.9:\n",
    "            self.model.stop_training = True\n",
    "            print(\"Reached 90% accuracy on both training and validation, stopping training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "quantitative-mauritius",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 1000\n",
    "TRAINING_SPLIT = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "flying-lincoln",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the data: (2225, 2)\n",
      "['tech'\n",
      " 'tv future in the hands of viewers with home theatre systems  plasma high-definition tvs  and digital video recorders moving into the living room  the way people watch tv will be radically different in five years  time.  that is according to an expert panel which gathered at the annual consumer electronics show in las vegas to discuss how these new technologies will impact one of our favourite pastimes. with the us leading the trend  programmes and other content will be delivered to viewers via home networks  through cable  satellite  telecoms companies  and broadband service providers to front rooms and portable devices.  one of the most talked-about technologies of ces has been digital and personal video recorders (dvr and pvr). these set-top boxes  like the us s tivo and the uk s sky+ system  allow people to record  store  play  pause and forward wind tv programmes when they want.  essentially  the technology allows for much more personalised tv. they are also being built-in to high-definition tv sets  which are big business in japan and the us  but slower to take off in europe because of the lack of high-definition programming. not only can people forward wind through adverts  they can also forget about abiding by network and channel schedules  putting together their own a-la-carte entertainment. but some us networks and cable and satellite companies are worried about what it means for them in terms of advertising revenues as well as  brand identity  and viewer loyalty to channels. although the us leads in this technology at the moment  it is also a concern that is being raised in europe  particularly with the growing uptake of services like sky+.  what happens here today  we will see in nine months to a years  time in the uk   adam hume  the bbc broadcast s futurologist told the bbc news website. for the likes of the bbc  there are no issues of lost advertising revenue yet. it is a more pressing issue at the moment for commercial uk broadcasters  but brand loyalty is important for everyone.  we will be talking more about content brands rather than network brands   said tim hanlon  from brand communications firm starcom mediavest.  the reality is that with broadband connections  anybody can be the producer of content.  he added:  the challenge now is that it is hard to promote a programme with so much choice.   what this means  said stacey jolna  senior vice president of tv guide tv group  is that the way people find the content they want to watch has to be simplified for tv viewers. it means that networks  in us terms  or channels could take a leaf out of google s book and be the search engine of the future  instead of the scheduler to help people find what they want to watch. this kind of channel model might work for the younger ipod generation which is used to taking control of their gadgets and what they play on them. but it might not suit everyone  the panel recognised. older generations are more comfortable with familiar schedules and channel brands because they know what they are getting. they perhaps do not want so much of the choice put into their hands  mr hanlon suggested.  on the other end  you have the kids just out of diapers who are pushing buttons already - everything is possible and available to them   said mr hanlon.  ultimately  the consumer will tell the market they want.   of the 50 000 new gadgets and technologies being showcased at ces  many of them are about enhancing the tv-watching experience. high-definition tv sets are everywhere and many new models of lcd (liquid crystal display) tvs have been launched with dvr capability built into them  instead of being external boxes. one such example launched at the show is humax s 26-inch lcd tv with an 80-hour tivo dvr and dvd recorder. one of the us s biggest satellite tv companies  directtv  has even launched its own branded dvr at the show with 100-hours of recording capability  instant replay  and a search function. the set can pause and rewind tv for up to 90 hours. and microsoft chief bill gates announced in his pre-show keynote speech a partnership with tivo  called tivotogo  which means people can play recorded programmes on windows pcs and mobile devices. all these reflect the increasing trend of freeing up multimedia so that people can watch what they want  when they want.']\n",
      "['business'\n",
      " 'worldcom boss  left books alone  former worldcom boss bernie ebbers  who is accused of overseeing an $11bn (Â£5.8bn) fraud  never made accounting decisions  a witness has told jurors.  david myers made the comments under questioning by defence lawyers who have been arguing that mr ebbers was not responsible for worldcom s problems. the phone company collapsed in 2002 and prosecutors claim that losses were hidden to protect the firm s shares. mr myers has already pleaded guilty to fraud and is assisting prosecutors.  on monday  defence lawyer reid weingarten tried to distance his client from the allegations. during cross examination  he asked mr myers if he ever knew mr ebbers  make an accounting decision  .  not that i am aware of   mr myers replied.  did you ever know mr ebbers to make an accounting entry into worldcom books   mr weingarten pressed.  no   replied the witness. mr myers has admitted that he ordered false accounting entries at the request of former worldcom chief financial officer scott sullivan. defence lawyers have been trying to paint mr sullivan  who has admitted fraud and will testify later in the trial  as the mastermind behind worldcom s accounting house of cards.  mr ebbers  team  meanwhile  are looking to portray him as an affable boss  who by his own admission is more pe graduate than economist. whatever his abilities  mr ebbers transformed worldcom from a relative unknown into a $160bn telecoms giant and investor darling of the late 1990s. worldcom s problems mounted  however  as competition increased and the telecoms boom petered out. when the firm finally collapsed  shareholders lost about $180bn and 20 000 workers lost their jobs. mr ebbers  trial is expected to last two months and if found guilty the former ceo faces a substantial jail sentence. he has firmly declared his innocence.']\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"data/articles.csv\"\n",
    "data = np.loadtxt(data_dir, delimiter=',', skiprows=1, dtype='str', comments=None)\n",
    "max_length = max(len(content.split()) for content in data[:,1])\n",
    "print(f\"Shape of the data: {data.shape}\")\n",
    "print(f\"{data[0]}\\n{data[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "small-violence",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "def train_val_datasets(data):\n",
    "    '''\n",
    "    Splits data into traning and validations sets\n",
    "    \n",
    "    Args:\n",
    "        data (np.array): array with two columns, first one is the label, the second is the text\n",
    "    \n",
    "    Returns:\n",
    "        (tf.data.Dataset, tf.data.Dataset): tuple containing the train and validation datasets\n",
    "    '''\n",
    "    TRAINING_SPLIT = 0.8 \n",
    "    \n",
    "    \n",
    "    train_size = int(len(data) * TRAINING_SPLIT)\n",
    "\n",
    "    texts = [data[i][1] for i in range(len(data))]\n",
    "    labels = [data[i][0] for i in range(len(data))]\n",
    "\n",
    "    train_texts = texts[:train_size]\n",
    "    validation_texts = texts[train_size:]\n",
    "    train_labels = labels[:train_size]\n",
    "    validation_labels = labels[train_size:]\n",
    "    \n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((train_texts, train_labels))\n",
    "    validation_dataset = tf.data.Dataset.from_tensor_slices((validation_texts, validation_labels))\n",
    "    \n",
    "    \n",
    "    return train_dataset, validation_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "circular-venue",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1780 sentence-label pairs for training.\n",
      "\n",
      "There are 445 sentence-label pairs for validation.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_dataset, validation_dataset = train_val_datasets(data)\n",
    "\n",
    "print(f\"There are {train_dataset.cardinality()} sentence-label pairs for training.\\n\")\n",
    "print(f\"There are {validation_dataset.cardinality()} sentence-label pairs for validation.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b87dbce-06a2-43b0-b098-b23597101645",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "def standardize_func(sentence):\n",
    "    \"\"\"\n",
    "    Removes a list of stopwords\n",
    "    \n",
    "    Args:\n",
    "        sentence (tf.string): sentence to remove the stopwords from\n",
    "    \n",
    "    Returns:\n",
    "        sentence (tf.string): lowercase sentence without the stopwords\n",
    "    \"\"\"\n",
    "    stopwords = [\"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \n",
    "                 \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\",\n",
    "                 \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\",\n",
    "                 \"have\", \"having\", \"he\", \"her\", \"here\",  \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\",  \"i\", \"if\",\n",
    "                 \"in\", \"into\", \"is\", \"it\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\",\n",
    "                 \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \n",
    "                 \"she\",  \"should\", \"so\", \"some\", \"such\", \"than\", \"that\",  \"the\", \"their\", \"theirs\", \"them\", \"themselves\",\n",
    "                 \"then\", \"there\", \"these\", \"they\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\",\n",
    "                 \"was\", \"we\",  \"were\", \"what\",  \"when\", \"where\", \"which\", \"while\", \"who\", \"whom\", \"why\", \"why\", \"with\", \"would\",\n",
    "                 \"you\",  \"your\", \"yours\", \"yourself\", \"yourselves\", \"'m\",  \"'d\", \"'ll\", \"'re\", \"'ve\", \"'s\", \"'d\"]\n",
    " \n",
    "    sentence = tf.strings.lower(sentence)\n",
    "    \n",
    "    for word in stopwords:\n",
    "        if word[0] == \"'\":\n",
    "            sentence = tf.strings.regex_replace(sentence, rf\"{word}\\b\", \"\")\n",
    "        else:\n",
    "            sentence = tf.strings.regex_replace(sentence, rf\"\\b{word}\\b\", \"\")\n",
    "    \n",
    "    sentence = tf.strings.regex_replace(sentence, r'[!\"#$%&()\\*\\+,-\\./:;<=>?@\\[\\\\\\]^_`{|}~\\']', \"\")\n",
    "\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "recreational-prince",
   "metadata": {
    "deletable": false,
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def fit_vectorizer(train_sentences, standardize_func):\n",
    "    '''\n",
    "    Defines and adapts the text vectorizer\n",
    "\n",
    "    Args:\n",
    "        train_sentences (tf.data.Dataset): sentences from the train dataset to fit the TextVectorization layer\n",
    "        standardize_func (FunctionType): function to remove stopwords and punctuation, and lowercase texts.\n",
    "    Returns:\n",
    "        TextVectorization: adapted instance of TextVectorization layer\n",
    "    '''\n",
    "    \n",
    "    vectorizer = tf.keras.layers.TextVectorization( \n",
    "\t\tstandardize=\"lower_and_strip_punctuation\",\n",
    "\t\tmax_tokens=VOCAB_SIZE,\n",
    "        output_sequence_length=max_length,\n",
    "\t) \n",
    "    \n",
    "    train_sentences = train_sentences.map(lambda x: standardize_func(x))\n",
    "    \n",
    "    \n",
    "    vectorizer.adapt(train_sentences)\n",
    "    \n",
    "    return vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "great-trading",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary contains 1000 words\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_only_dataset = train_dataset.map(lambda text, label: text)\n",
    "vectorizer = fit_vectorizer(text_only_dataset, standardize_func)\n",
    "vocab_size = vectorizer.vocabulary_size()\n",
    "\n",
    "print(f\"Vocabulary contains {vocab_size} words\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "active-objective",
   "metadata": {
    "deletable": false,
    "id": "XkWiQ_FKZNp2",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def fit_label_encoder(train_labels, validation_labels):\n",
    "    \"\"\"Creates an instance of a StringLookup, and trains it on all labels\n",
    "\n",
    "    Args:\n",
    "        train_labels (tf.data.Dataset): dataset of train labels\n",
    "        validation_labels (tf.data.Dataset): dataset of validation labels\n",
    "\n",
    "    Returns:\n",
    "        tf.keras.layers.StringLookup: adapted encoder for train and validation labels\n",
    "    \"\"\"\n",
    "    \n",
    "    labels = train_labels.concatenate(validation_labels)\n",
    "\n",
    "    label_encoder = tf.keras.layers.StringLookup(num_oov_indices=0)\n",
    "    \n",
    "    label_encoder.adapt(labels)\n",
    "   \n",
    "    \n",
    "    return label_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c6a7e5a-ea50-4663-8062-d076dcd5313f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique labels: ['sport', 'business', 'politics', 'tech', 'entertainment']\n"
     ]
    }
   ],
   "source": [
    "train_labels_only = train_dataset.map(lambda text, label: label)\n",
    "validation_labels_only = validation_dataset.map(lambda text, label: label)\n",
    "\n",
    "label_encoder = fit_label_encoder(train_labels_only,validation_labels_only)\n",
    "                                  \n",
    "print(f'Unique labels: {label_encoder.get_vocabulary()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fourth-knight",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "def preprocess_dataset(dataset, text_vectorizer, label_encoder):\n",
    "    \"\"\"Apply the preprocessing to a dataset\n",
    "\n",
    "    Args:\n",
    "        dataset (tf.data.Dataset): dataset to preprocess\n",
    "        text_vectorizer (tf.keras.layers.TextVectorization ): text vectorizer\n",
    "        label_encoder (tf.keras.layers.StringLookup): label encoder\n",
    "\n",
    "    Returns:\n",
    "        tf.data.Dataset: transformed dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    def preprocess_text_label(text, label):\n",
    "\n",
    "        text = text_vectorizer(text)\n",
    "        \n",
    "        label = tf.convert_to_tensor(label_encoder(label), dtype=tf.int64)\n",
    "        \n",
    "        return text, label\n",
    "\n",
    "    dataset = dataset.map(preprocess_text_label)\n",
    "    dataset = dataset.batch(32)\n",
    "\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "separate-onion",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches in the train dataset: 56\n",
      "Number of batches in the validation dataset: 14\n"
     ]
    }
   ],
   "source": [
    "train_proc_dataset = preprocess_dataset(train_dataset, vectorizer, label_encoder)\n",
    "validation_proc_dataset = preprocess_dataset(validation_dataset, vectorizer, label_encoder)\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Number of batches in the train dataset: {train_proc_dataset.cardinality()}\")\n",
    "print(f\"Number of batches in the validation dataset: {validation_proc_dataset.cardinality()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7975a5b2-2a09-4cdd-8eba-f8a54a3fcae3",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the train batch: (32, 4492)\n",
      "Shape of the validation batch: (32, 4492)\n"
     ]
    }
   ],
   "source": [
    "train_batch = next(train_proc_dataset.as_numpy_iterator())\n",
    "validation_batch = next(validation_proc_dataset.as_numpy_iterator())\n",
    "\n",
    "print(f\"Shape of the train batch: {train_batch[0].shape}\")\n",
    "print(f\"Shape of the validation batch: {validation_batch[0].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "little-bahrain",
   "metadata": {
    "deletable": false,
    "id": "HZ5um4MWZP-W"
   },
   "outputs": [],
   "source": [
    "FILTERS = 320\n",
    "KERNEL_SIZE = 5\n",
    "LSTM_UNITS =128\n",
    "EMBEDDING_DIM = 24\n",
    "def create_model(EMBEDDING_DIM = EMBEDDING_DIM,FILTERS = FILTERS, KERNEL_SIZE = KERNEL_SIZE, act = 'relu', l1 = 256, drop = 0.4):\n",
    "    \"\"\"\n",
    "    Creates a text classifier model\n",
    "    Returns:\n",
    "      tf.keras Model: the text classifier model\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "\t\n",
    "    model = tf.keras.Sequential([ \n",
    "        tf.keras.Input(shape = (max_length,)),\n",
    "        tf.keras.layers.Embedding(VOCAB_SIZE, EMBEDDING_DIM),\n",
    "        # tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units=LSTM_UNITS, return_sequences=True)),\n",
    "        tf.keras.layers.Conv1D(FILTERS, KERNEL_SIZE-2, activation='relu'),\n",
    "        tf.keras.layers.Conv1D(FILTERS*2, KERNEL_SIZE, activation='relu'),\n",
    "        # tf.keras.layers.Conv1D(FILTERS*2, KERNEL_SIZE, activation='relu'),\n",
    "        tf.keras.layers.GlobalAveragePooling1D(), \n",
    "        tf.keras.layers.Dropout(drop),\n",
    "        tf.keras.layers.Dense(l1, activation=act),\n",
    "        tf.keras.layers.Dense(5,activation ='linear')\n",
    "    ])\n",
    "    \n",
    "\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f442c92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_comb = []\n",
    "\n",
    "def grid_search():\n",
    "    Filters = [16,32,64]\n",
    "    Kernel = [ 3,4,5,6]\n",
    "    embed = [16,24,32,48]\n",
    "    l1 = [ 64,128,192,256]\n",
    "    act1 = ['relu','sigmoid']\n",
    "    drop = [0.2,0.3,0.4]\n",
    "    for f in Filters :\n",
    "        for k in Kernel:\n",
    "            for e in embed : \n",
    "                for l in l1 :\n",
    "                    for act in act1 :\n",
    "                        for d in drop : \n",
    "                            best_param = [f,k,e,l,act,d]\n",
    "                            print(best_param)\n",
    "                            model = create_model(EMBEDDING_DIM=e,KERNEL_SIZE=k,FILTERS=f,l1 = l,act =act,drop = d)\n",
    "                            model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'],from_logits=True)\n",
    "                            history = model.fit(train_proc_dataset, epochs=20, validation_data=validation_proc_dataset,callbacks=[EarlyStoppingCallback(patience = 2)],verbose= 0 )\n",
    "                            print (history.history['accuracy'][-1], history.history[f'val_accuracy'][-1]>=0.85)\n",
    "                            if history.history['accuracy'][-1] >= 0.85 and history.history[f'val_accuracy'][-1]>=0.85 : \n",
    "                                print(\"--------------------------------------SAVED PARAMETERS--------------------------------\")\n",
    "                                best_comb.append(best_param)\n",
    "    return best_comb\n",
    "\n",
    "def objective(trial):\n",
    "    # Define hyperparameters to tune\n",
    "    f = trial.suggest_categorical('f',[16,32,64])\n",
    "    k = trial.suggest_categorical('k',[ 3,4,5,6])\n",
    "    e = trial.suggest_categorical('e',[16,24,32,48])\n",
    "    l = trial.suggest_categorical('l',[ 64,128,192,256])\n",
    "    act = trial.suggest_categorical('act',['relu','sigmoid'])\n",
    "    d = trial.suggest_categorical('d',[0.2,0.3,0.4,0.5])\n",
    "\n",
    "\n",
    "    model = create_model(EMBEDDING_DIM=e,KERNEL_SIZE=k,FILTERS=f,l1 = l,act =act,drop = d)\n",
    "    model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
    "    history = model.fit(train_proc_dataset, epochs=20, validation_data=validation_proc_dataset,callbacks=[EarlyStoppingCallback(patience = 2)],verbose= 0 )\n",
    "    print()\n",
    "    print(f,k,e,l,act,d)\n",
    "    print (history.history['accuracy'][-1], history.history[f'val_accuracy'][-1])\n",
    "    print('--------------------------------------------------------------------------------------------------------------')\n",
    "    if history.history['accuracy'][-1] >= 0.85 and history.history[f'val_accuracy'][-1]>=0.85 : \n",
    "        best_param = [f,k,e,l,act,d]\n",
    "        score = [history.history['accuracy'],history.history[f'val_accuracy'][-1]]\n",
    "        f = best_param,score\n",
    "        best_comb.append(f)\n",
    "        print(\"--------------------------------------SAVED PARAMETERS--------------------------------\")\n",
    "\n",
    "    return history.history['val_accuracy'][-1]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b3825038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with tf.device('/GPU:0'):\n",
    "#     study = optuna.create_study(direction='maximize')  \n",
    "#     study.optimize(objective, n_trials=100) \n",
    "#     best_params =grid_search()\n",
    "# # f': 64, 'k': 4, 'e': 48, 'l': 192, 'act': 'sigmoid', 'd': 0.3}\n",
    "# print(\"Best optuna hyperparameters: \", study.best_params)         \n",
    "# print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "resident-productivity",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 4492, 16)          16000     \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 4490, 300)         14700     \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 4486, 600)         900600    \n",
      "                                                                 \n",
      " global_average_pooling1d (G  (None, 600)              0         \n",
      " lobalAveragePooling1D)                                          \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 600)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                38464     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 5)                 325       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 970,089\n",
      "Trainable params: 970,089\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model(EMBEDDING_DIM=16, FILTERS=300, KERNEL_SIZE=5, l1=64, act='relu', drop=0.3)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c6f1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(model, min_lr=1e-6, max_lr=1, epochs=100):\n",
    "    \"\"\"Fit model using different learning rates to find optimal learning rate\n",
    "    \n",
    "    Args:\n",
    "        model (tf.keras.Model): uncompiled model\n",
    "        min_lr (float): minimum learning rate to test\n",
    "        max_lr (float): maximum learning rate to test\n",
    "        epochs (int): number of epochs to train\n",
    "        \n",
    "    Returns:\n",
    "        tf.keras.callbacks.History: callback history\n",
    "    \"\"\"\n",
    "    \n",
    "    mult_factor = (max_lr / min_lr) ** (1/epochs)\n",
    "    \n",
    "    lr_schedule = tf.keras.callbacks.LearningRateScheduler(\n",
    "        lambda epoch: min_lr * (mult_factor ** epoch)\n",
    "    )\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(min_lr)\n",
    "    \n",
    "    model.compile(\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        optimizer=optimizer,\n",
    "        metrics=['accuracy'],\n",
    "        \n",
    "    )\n",
    "    \n",
    "    history = model.fit(\n",
    "        train_proc_dataset,\n",
    "        epochs=epochs,\n",
    "        callbacks=[lr_schedule],\n",
    "        validation_data=validation_proc_dataset,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    return history\n",
    "\n",
    "\n",
    "\n",
    "# lr_history = adjust_learning_rate(model=model)\n",
    "\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.semilogx(lr_history.history[\"lr\"], lr_history.history[\"loss\"])\n",
    "# plt.grid(True)\n",
    "# plt.xlabel(\"Learning Rate\")\n",
    "# plt.ylabel(\"Loss\")\n",
    "# plt.title(\"Learning Rate vs Loss\")\n",
    "\n",
    "# plt.xlim([1e-7, 1])\n",
    "# plt.ylim([0, max(lr_history.history[\"loss\"])])\n",
    "\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.semilogx(lr_history.history[\"lr\"], lr_history.history[\"accuracy\"])\n",
    "# plt.grid(True)\n",
    "# plt.xlabel(\"Learning Rate\")\n",
    "# plt.ylabel(\"Accuracy\")\n",
    "# plt.title(\"Learning Rate vs Accuracy\")\n",
    "# plt.xlim([1e-7, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "498bf653",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "24/56 [===========>..................] - ETA: 9s - loss: 1.6089 - accuracy: 0.2161"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_cell_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtimeit\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m-n1 -r1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwith tf.device(\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m/GPU:0\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m):\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    lr = 5e-3\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    model.compile(\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        optimizer=tf.keras.optimizers.Adam(learning_rate = lr),\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        metrics=[\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43maccuracy\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m]\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    )\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    history = model.fit(train_proc_dataset, epochs=50, validation_data=validation_proc_dataset)#,callbacks=[EarlyStoppingCallback(patience = 3)])\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mdef plot_graphs(history, metric):\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    plt.plot(history.history[metric])\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    plt.plot(history.history[f\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mval_\u001b[39;49m\u001b[38;5;132;43;01m{metric}\u001b[39;49;00m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m])\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    plt.xlabel(\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEpochs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    plt.ylabel(metric)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    plt.legend([metric, f\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mval_\u001b[39;49m\u001b[38;5;132;43;01m{metric}\u001b[39;49;00m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m])\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    plt.show()\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mplot_graphs(history=history, metric=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maccuracy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mplot_graphs(history=history, metric=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mloss\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\interactiveshell.py:2541\u001b[0m, in \u001b[0;36mInteractiveShell.run_cell_magic\u001b[1;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[0;32m   2539\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuiltin_trap:\n\u001b[0;32m   2540\u001b[0m     args \u001b[38;5;241m=\u001b[39m (magic_arg_s, cell)\n\u001b[1;32m-> 2541\u001b[0m     result \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   2543\u001b[0m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[0;32m   2544\u001b[0m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[0;32m   2545\u001b[0m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[0;32m   2546\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic\u001b[38;5;241m.\u001b[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\magics\\execution.py:1189\u001b[0m, in \u001b[0;36mExecutionMagics.timeit\u001b[1;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[0;32m   1186\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m time_number \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.2\u001b[39m:\n\u001b[0;32m   1187\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m-> 1189\u001b[0m all_runs \u001b[38;5;241m=\u001b[39m \u001b[43mtimer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepeat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepeat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1190\u001b[0m best \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(all_runs) \u001b[38;5;241m/\u001b[39m number\n\u001b[0;32m   1191\u001b[0m worst \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(all_runs) \u001b[38;5;241m/\u001b[39m number\n",
      "File \u001b[1;32mc:\\Program Files\\Python310\\lib\\timeit.py:206\u001b[0m, in \u001b[0;36mTimer.repeat\u001b[1;34m(self, repeat, number)\u001b[0m\n\u001b[0;32m    204\u001b[0m r \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(repeat):\n\u001b[1;32m--> 206\u001b[0m     t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnumber\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    207\u001b[0m     r\u001b[38;5;241m.\u001b[39mappend(t)\n\u001b[0;32m    208\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\magics\\execution.py:173\u001b[0m, in \u001b[0;36mTimer.timeit\u001b[1;34m(self, number)\u001b[0m\n\u001b[0;32m    171\u001b[0m gc\u001b[38;5;241m.\u001b[39mdisable()\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 173\u001b[0m     timing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    175\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gcold:\n",
      "File \u001b[1;32m<magic-timeit>:8\u001b[0m, in \u001b[0;36minner\u001b[1;34m(_it, _timer)\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training.py:1564\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1556\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1558\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1561\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1562\u001b[0m ):\n\u001b[0;32m   1563\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1564\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1565\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1566\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Program Files\\Python310\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Program Files\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Program Files\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateless_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Program Files\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2493\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   2494\u001b[0m   (graph_function,\n\u001b[0;32m   2495\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Program Files\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1858\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1859\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1860\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1861\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1862\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1863\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1864\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1865\u001b[0m     args,\n\u001b[0;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1867\u001b[0m     executing_eagerly)\n\u001b[0;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Program Files\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Program Files\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%timeit -n1 -r1\n",
    "with tf.device('/GPU:0'):\n",
    "    lr = 5e-3\n",
    "    model.compile(\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate = lr),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    history = model.fit(train_proc_dataset, epochs=50, validation_data=validation_proc_dataset)#,callbacks=[EarlyStoppingCallback(patience = 3)])\n",
    "\n",
    "\n",
    "def plot_graphs(history, metric):\n",
    "    plt.plot(history.history[metric])\n",
    "    plt.plot(history.history[f'val_{metric}'])\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(metric)\n",
    "    plt.legend([metric, f'val_{metric}'])\n",
    "    plt.show()\n",
    "    \n",
    "plot_graphs(history=history, metric=\"accuracy\")\n",
    "plot_graphs(history=history, metric=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "30c10ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(sentence):\n",
    "    \"\"\"\n",
    "    Predict the label for a given sentence\n",
    "\n",
    "    Args:\n",
    "        sentence (str): input sentence to classify\n",
    "        model (tf.keras.Model): trained text classification model\n",
    "\n",
    "    Returns:\n",
    "        str: predicted label for the input sentence\n",
    "    \"\"\"\n",
    "\n",
    "    sentence = tf.expand_dims(input=sentence, axis=0)\n",
    "    sentence = vectorizer(sentence)\n",
    "    \n",
    "    predictions = model.predict(sentence)\n",
    "    prediction = tf.nn.softmax(predictions).numpy()\n",
    "    print(prediction)\n",
    "\n",
    "    predicted_label_index = tf.argmax(input=prediction, axis=1).numpy()[0]\n",
    "    \n",
    "    predicted_label = label_encoder.get_vocabulary()[predicted_label_index]\n",
    "    \n",
    "    return predicted_label, np.max(prediction)*100\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c13234e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(model):\n",
    "    with open(file='model.pkl', mode='wb') as f:\n",
    "        pickle.dump(obj=model, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cb3069d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def  load_model():\n",
    "    with open(file='model.pkl', mode ='rb') as f:\n",
    "        model = pickle.load(file=f)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "60981c6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://6157f70a-2384-479a-bc3a-d978ce0ac3d6/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://6157f70a-2384-479a-bc3a-d978ce0ac3d6/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 90ms/step\n",
      "[[0.0000000e+00 1.3855459e-17 2.3063411e-18 1.0000000e+00 4.8617620e-33]]\n",
      "The predicted label for the sentence is: tech with a probability of 100.0000%\n"
     ]
    }
   ],
   "source": [
    "save(model=model)\n",
    "cont = True\n",
    "while cont:\n",
    "    sentence = input(prompt=\"Enter a sentence to classify, N to stop :\")\n",
    "    if sentence.capitalize().strip() == \"N\":\n",
    "        cont = False \n",
    "        break\n",
    "    \n",
    "    predicted_label,proba = predict(sentence=sentence)\n",
    "    print(f\"The predicted label for the sentence is: {predicted_label} with a probability of {proba:.4f}%\")"
   ]
  }
 ],
 "metadata": {
  "dlai_version": "1.2.0",
  "grader_version": "1",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

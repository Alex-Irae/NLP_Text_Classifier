{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc4cd555",
   "metadata": {},
   "source": [
    "# NLP Text Classifier\n",
    "\n",
    "This project implements a text classification model using TensorFlow. The model preprocesses text data, removes stopwords, and trains a neural network to classify text into predefined categories.\n",
    "\n",
    "## Project Structure\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931d2b47",
   "metadata": {
    "vscode": {
     "languageId": ""
    }
   },
   "outputs": [],
   "source": [
    "data/\n",
    "    articles.csv\n",
    "main.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c035c987",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "- articles.csv : Contains the dataset used for training and validation.\n",
    "- main.ipynb : Jupyter notebook containing the code for preprocessing, training, and evaluating the text classification model.\n",
    "\n",
    "## Setup\n",
    "\n",
    "1. Clone the repository.\n",
    "2. Install the required dependencies:\n",
    "    ```sh\n",
    "    pip install -r requirements.txt\n",
    "    ```\n",
    "\n",
    "## Usage\n",
    "\n",
    "### Preprocessing\n",
    "\n",
    "The fit_vectorizer function defines and adapts a text vectorizer to preprocess the text data by removing stopwords and punctuation, and converting the text to lowercase.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f443f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_vectorizer(train_sentences, standardize_func):\n",
    "    '''\n",
    "    Defines and adapts the text vectorizer\n",
    "\n",
    "    Args:\n",
    "        train_sentences (tf.data.Dataset): sentences from the train dataset to fit the TextVectorization layer\n",
    "        standardize_func (FunctionType): function to remove stopwords and punctuation, and lowercase texts.\n",
    "    Returns:\n",
    "        TextVectorization: adapted instance of TextVectorization layer\n",
    "    '''\n",
    "    \n",
    "    vectorizer = tf.keras.layers.TextVectorization( \n",
    "        standardize=\"lower_and_strip_punctuation\",\n",
    "        max_tokens=VOCAB_SIZE,\n",
    "        output_sequence_length=max_length,\n",
    "    ) \n",
    "    \n",
    "    train_sentences = train_sentences.map(lambda x: standardize_func(x))\n",
    "    \n",
    "    vectorizer.adapt(train_sentences)\n",
    "    \n",
    "    return vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0b9abb",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Training\n",
    "\n",
    "The notebook includes functions to split the dataset into training and validation sets, preprocess the data, and train the model using different hyperparameters.\n",
    "\n",
    "### Evaluation\n",
    "\n",
    "The model's performance is evaluated using accuracy and loss metrics. The notebook includes functions to plot these metrics and visualize the learning rate vs. loss and accuracy.\n",
    "\n",
    "## Running the Notebook\n",
    "\n",
    "1. Open main.ipynb in Jupyter Notebook or JupyterLab.\n",
    "2. Run the cells sequentially to preprocess the data, train the model, and evaluate its performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6fef9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c6c4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStoppingCallback(tf.keras.callbacks.Callback):\n",
    "    \"\"\"Stop training once accuracy reaches 90% on both training and validation sets, with a patience mechanism.\"\"\"\n",
    "    \n",
    "    def __init__(self, patience=2):\n",
    "        super(EarlyStoppingCallback, self).__init__()\n",
    "        self.patience = patience\n",
    "        self.best_val_accuracy = 0\n",
    "        self.wait = 0  \n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        current_val_accuracy = logs.get('val_accuracy')  \n",
    "\n",
    "        if current_val_accuracy is not None:\n",
    "            if current_val_accuracy > self.best_val_accuracy:\n",
    "                self.best_val_accuracy = current_val_accuracy \n",
    "                self.wait = 0 \n",
    "            else:\n",
    "                self.wait += 1  \n",
    "\n",
    "            if self.wait >= self.patience:\n",
    "                self.model.stop_training = True\n",
    "                print(f\"Early stopping: validation accuracy did not improve for {self.patience} epochs.\")\n",
    "\n",
    "        if logs.get('accuracy') >= 0.9 and current_val_accuracy >= 0.9:\n",
    "            self.model.stop_training = True\n",
    "            print(\"Reached 90% accuracy on both training and validation, stopping training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quantitative-mauritius",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 1000\n",
    "TRAINING_SPLIT = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flying-lincoln",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_dir = \"data/articles.csv\"\n",
    "data = np.loadtxt(data_dir, delimiter=',', skiprows=1, dtype='str', comments=None)\n",
    "max_length = max(len(content.split()) for content in data[:,1])\n",
    "print(f\"Shape of the data: {data.shape}\")\n",
    "print(f\"{data[0]}\\n{data[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "small-violence",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "def train_val_datasets(data):\n",
    "    '''\n",
    "    Splits data into traning and validations sets\n",
    "    \n",
    "    Args:\n",
    "        data (np.array): array with two columns, first one is the label, the second is the text\n",
    "    \n",
    "    Returns:\n",
    "        (tf.data.Dataset, tf.data.Dataset): tuple containing the train and validation datasets\n",
    "    '''\n",
    "    TRAINING_SPLIT = 0.8 \n",
    "    \n",
    "    \n",
    "    train_size = int(len(data) * TRAINING_SPLIT)\n",
    "\n",
    "    texts = [data[i][1] for i in range(len(data))]\n",
    "    labels = [data[i][0] for i in range(len(data))]\n",
    "\n",
    "    train_texts = texts[:train_size]\n",
    "    validation_texts = texts[train_size:]\n",
    "    train_labels = labels[:train_size]\n",
    "    validation_labels = labels[train_size:]\n",
    "    \n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((train_texts, train_labels))\n",
    "    validation_dataset = tf.data.Dataset.from_tensor_slices((validation_texts, validation_labels))\n",
    "    \n",
    "    \n",
    "    return train_dataset, validation_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "circular-venue",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset, validation_dataset = train_val_datasets(data)\n",
    "\n",
    "print(f\"There are {train_dataset.cardinality()} sentence-label pairs for training.\\n\")\n",
    "print(f\"There are {validation_dataset.cardinality()} sentence-label pairs for validation.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b87dbce-06a2-43b0-b098-b23597101645",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "def standardize_func(sentence):\n",
    "    \"\"\"\n",
    "    Removes a list of stopwords\n",
    "    \n",
    "    Args:\n",
    "        sentence (tf.string): sentence to remove the stopwords from\n",
    "    \n",
    "    Returns:\n",
    "        sentence (tf.string): lowercase sentence without the stopwords\n",
    "    \"\"\"\n",
    "    stopwords = [\"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \n",
    "                 \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\",\n",
    "                 \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\",\n",
    "                 \"have\", \"having\", \"he\", \"her\", \"here\",  \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\",  \"i\", \"if\",\n",
    "                 \"in\", \"into\", \"is\", \"it\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\",\n",
    "                 \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \n",
    "                 \"she\",  \"should\", \"so\", \"some\", \"such\", \"than\", \"that\",  \"the\", \"their\", \"theirs\", \"them\", \"themselves\",\n",
    "                 \"then\", \"there\", \"these\", \"they\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\",\n",
    "                 \"was\", \"we\",  \"were\", \"what\",  \"when\", \"where\", \"which\", \"while\", \"who\", \"whom\", \"why\", \"why\", \"with\", \"would\",\n",
    "                 \"you\",  \"your\", \"yours\", \"yourself\", \"yourselves\", \"'m\",  \"'d\", \"'ll\", \"'re\", \"'ve\", \"'s\", \"'d\"]\n",
    " \n",
    "    sentence = tf.strings.lower(sentence)\n",
    "    \n",
    "    for word in stopwords:\n",
    "        if word[0] == \"'\":\n",
    "            sentence = tf.strings.regex_replace(sentence, rf\"{word}\\b\", \"\")\n",
    "        else:\n",
    "            sentence = tf.strings.regex_replace(sentence, rf\"\\b{word}\\b\", \"\")\n",
    "    \n",
    "    sentence = tf.strings.regex_replace(sentence, r'[!\"#$%&()\\*\\+,-\\./:;<=>?@\\[\\\\\\]^_`{|}~\\']', \"\")\n",
    "\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recreational-prince",
   "metadata": {
    "deletable": false,
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def fit_vectorizer(train_sentences, standardize_func):\n",
    "    '''\n",
    "    Defines and adapts the text vectorizer\n",
    "\n",
    "    Args:\n",
    "        train_sentences (tf.data.Dataset): sentences from the train dataset to fit the TextVectorization layer\n",
    "        standardize_func (FunctionType): function to remove stopwords and punctuation, and lowercase texts.\n",
    "    Returns:\n",
    "        TextVectorization: adapted instance of TextVectorization layer\n",
    "    '''\n",
    "    \n",
    "    vectorizer = tf.keras.layers.TextVectorization( \n",
    "\t\tstandardize=\"lower_and_strip_punctuation\",\n",
    "\t\tmax_tokens=VOCAB_SIZE,\n",
    "        output_sequence_length=max_length,\n",
    "\t) \n",
    "    \n",
    "    train_sentences = train_sentences.map(lambda x: standardize_func(x))\n",
    "    \n",
    "    \n",
    "    vectorizer.adapt(train_sentences)\n",
    "    \n",
    "    return vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "great-trading",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_only_dataset = train_dataset.map(lambda text, label: text)\n",
    "vectorizer = fit_vectorizer(text_only_dataset, standardize_func)\n",
    "vocab_size = vectorizer.vocabulary_size()\n",
    "\n",
    "print(f\"Vocabulary contains {vocab_size} words\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "active-objective",
   "metadata": {
    "deletable": false,
    "id": "XkWiQ_FKZNp2",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def fit_label_encoder(train_labels, validation_labels):\n",
    "    \"\"\"Creates an instance of a StringLookup, and trains it on all labels\n",
    "\n",
    "    Args:\n",
    "        train_labels (tf.data.Dataset): dataset of train labels\n",
    "        validation_labels (tf.data.Dataset): dataset of validation labels\n",
    "\n",
    "    Returns:\n",
    "        tf.keras.layers.StringLookup: adapted encoder for train and validation labels\n",
    "    \"\"\"\n",
    "    \n",
    "    labels = train_labels.concatenate(validation_labels)\n",
    "\n",
    "    label_encoder = tf.keras.layers.StringLookup(num_oov_indices=0)\n",
    "    \n",
    "    label_encoder.adapt(labels)\n",
    "   \n",
    "    \n",
    "    return label_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6a7e5a-ea50-4663-8062-d076dcd5313f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_labels_only = train_dataset.map(lambda text, label: label)\n",
    "validation_labels_only = validation_dataset.map(lambda text, label: label)\n",
    "\n",
    "label_encoder = fit_label_encoder(train_labels_only,validation_labels_only)\n",
    "                                  \n",
    "print(f'Unique labels: {label_encoder.get_vocabulary()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fourth-knight",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "def preprocess_dataset(dataset, text_vectorizer, label_encoder):\n",
    "    \"\"\"Apply the preprocessing to a dataset\n",
    "\n",
    "    Args:\n",
    "        dataset (tf.data.Dataset): dataset to preprocess\n",
    "        text_vectorizer (tf.keras.layers.TextVectorization ): text vectorizer\n",
    "        label_encoder (tf.keras.layers.StringLookup): label encoder\n",
    "\n",
    "    Returns:\n",
    "        tf.data.Dataset: transformed dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    def preprocess_text_label(text, label):\n",
    "\n",
    "        text = text_vectorizer(text)\n",
    "        \n",
    "        label = tf.convert_to_tensor(label_encoder(label), dtype=tf.int64)\n",
    "        \n",
    "        return text, label\n",
    "\n",
    "    dataset = dataset.map(preprocess_text_label)\n",
    "    dataset = dataset.batch(32)\n",
    "\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "separate-onion",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_proc_dataset = preprocess_dataset(train_dataset, vectorizer, label_encoder)\n",
    "validation_proc_dataset = preprocess_dataset(validation_dataset, vectorizer, label_encoder)\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Number of batches in the train dataset: {train_proc_dataset.cardinality()}\")\n",
    "print(f\"Number of batches in the validation dataset: {validation_proc_dataset.cardinality()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7975a5b2-2a09-4cdd-8eba-f8a54a3fcae3",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "train_batch = next(train_proc_dataset.as_numpy_iterator())\n",
    "validation_batch = next(validation_proc_dataset.as_numpy_iterator())\n",
    "\n",
    "print(f\"Shape of the train batch: {train_batch[0].shape}\")\n",
    "print(f\"Shape of the validation batch: {validation_batch[0].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "little-bahrain",
   "metadata": {
    "deletable": false,
    "id": "HZ5um4MWZP-W"
   },
   "outputs": [],
   "source": [
    "FILTERS = 320\n",
    "KERNEL_SIZE = 5\n",
    "LSTM_UNITS =128\n",
    "EMBEDDING_DIM = 24\n",
    "def create_model(EMBEDDING_DIM = EMBEDDING_DIM,FILTERS = FILTERS, KERNEL_SIZE = KERNEL_SIZE, act = 'relu', l1 = 256, drop = 0.4):\n",
    "    \"\"\"\n",
    "    Creates a text classifier model\n",
    "    Returns:\n",
    "      tf.keras Model: the text classifier model\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "\t\n",
    "    model = tf.keras.Sequential([ \n",
    "        tf.keras.Input(shape = (max_length,)),\n",
    "        tf.keras.layers.Embedding(VOCAB_SIZE, EMBEDDING_DIM),\n",
    "        # tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units=LSTM_UNITS, return_sequences=True)),\n",
    "        tf.keras.layers.Conv1D(FILTERS, KERNEL_SIZE-2, activation='relu'),\n",
    "        tf.keras.layers.Conv1D(FILTERS*2, KERNEL_SIZE, activation='relu'),\n",
    "        # tf.keras.layers.Conv1D(FILTERS*2, KERNEL_SIZE, activation='relu'),\n",
    "        tf.keras.layers.GlobalAveragePooling1D(), \n",
    "        tf.keras.layers.Dropout(drop),\n",
    "        tf.keras.layers.Dense(l1, activation=act),\n",
    "        tf.keras.layers.Dense(5,activation ='softmax')\n",
    "    ])\n",
    "    \n",
    "\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f442c92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_comb = []\n",
    "\n",
    "def grid_search():\n",
    "    Filters = [16,32,64]\n",
    "    Kernel = [ 3,4,5,6]\n",
    "    embed = [16,24,32,48]\n",
    "    l1 = [ 64,128,192,256]\n",
    "    act1 = ['relu','sigmoid']\n",
    "    drop = [0.2,0.3,0.4]\n",
    "    for f in Filters :\n",
    "        for k in Kernel:\n",
    "            for e in embed : \n",
    "                for l in l1 :\n",
    "                    for act in act1 :\n",
    "                        for d in drop : \n",
    "                            best_param = [f,k,e,l,act,d]\n",
    "                            print(best_param)\n",
    "                            model = create_model(EMBEDDING_DIM=e,KERNEL_SIZE=k,FILTERS=f,l1 = l,act =act,drop = d)\n",
    "                            history = model.fit(train_proc_dataset, epochs=20, validation_data=validation_proc_dataset,callbacks=[EarlyStoppingCallback(patience = 2)],verbose= 0 )\n",
    "                            model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "                            print (history.history['accuracy'][-1], history.history[f'val_accuracy'][-1]>=0.85)\n",
    "                            if history.history['accuracy'][-1] >= 0.85 and history.history[f'val_accuracy'][-1]>=0.85 : \n",
    "                                print(\"--------------------------------------SAVED PARAMETERS--------------------------------\")\n",
    "                                best_comb.append(best_param)\n",
    "    return best_comb\n",
    "\n",
    "def objective(trial):\n",
    "    # Define hyperparameters to tune\n",
    "    f = trial.suggest_categorical('f',[16,32,64])\n",
    "    k = trial.suggest_categorical('k',[ 3,4,5,6])\n",
    "    e = trial.suggest_categorical('e',[16,24,32,48])\n",
    "    l = trial.suggest_categorical('l',[ 64,128,192,256])\n",
    "    act = trial.suggest_categorical('act',['relu','sigmoid'])\n",
    "    d = trial.suggest_categorical('d',[0.2,0.3,0.4,0.5])\n",
    "\n",
    "\n",
    "    model = create_model(EMBEDDING_DIM=e,KERNEL_SIZE=k,FILTERS=f,l1 = l,act =act,drop = d)\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    history = model.fit(train_proc_dataset, epochs=20, validation_data=validation_proc_dataset,callbacks=[EarlyStoppingCallback(patience = 2)],verbose= 0 )\n",
    "    print()\n",
    "    print(f,k,e,l,act,d)\n",
    "    print (history.history['accuracy'][-1], history.history[f'val_accuracy'][-1])\n",
    "    print('--------------------------------------------------------------------------------------------------------------')\n",
    "    if history.history['accuracy'][-1] >= 0.85 and history.history[f'val_accuracy'][-1]>=0.85 : \n",
    "        best_param = [f,k,e,l,act,d]\n",
    "        score = [history.history['accuracy'],history.history[f'val_accuracy'][-1]]\n",
    "        f = best_param,score\n",
    "        best_comb.append(f)\n",
    "        print(\"--------------------------------------SAVED PARAMETERS--------------------------------\")\n",
    "\n",
    "    return history.history['val_accuracy'][-1]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3825038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with tf.device('/GPU:0'):\n",
    "#     study = optuna.create_study(direction='maximize')  \n",
    "#     study.optimize(objective, n_trials=100) \n",
    "#     best_params =grid_search()\n",
    "# # f': 64, 'k': 4, 'e': 48, 'l': 192, 'act': 'sigmoid', 'd': 0.3}\n",
    "# print(\"Best optuna hyperparameters: \", study.best_params)         \n",
    "# print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "resident-productivity",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = create_model(EMBEDDING_DIM=16, FILTERS=300, KERNEL_SIZE=5, l1=64, act='relu', drop=0.3)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c6f1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(model, min_lr=1e-6, max_lr=1, epochs=100):\n",
    "    \"\"\"Fit model using different learning rates to find optimal learning rate\n",
    "    \n",
    "    Args:\n",
    "        model (tf.keras.Model): uncompiled model\n",
    "        min_lr (float): minimum learning rate to test\n",
    "        max_lr (float): maximum learning rate to test\n",
    "        epochs (int): number of epochs to train\n",
    "        \n",
    "    Returns:\n",
    "        tf.keras.callbacks.History: callback history\n",
    "    \"\"\"\n",
    "    \n",
    "    mult_factor = (max_lr / min_lr) ** (1/epochs)\n",
    "    \n",
    "    lr_schedule = tf.keras.callbacks.LearningRateScheduler(\n",
    "        lambda epoch: min_lr * (mult_factor ** epoch)\n",
    "    )\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(min_lr)\n",
    "    \n",
    "    model.compile(\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        optimizer=optimizer,\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    history = model.fit(\n",
    "        train_proc_dataset,\n",
    "        epochs=epochs,\n",
    "        callbacks=[lr_schedule],\n",
    "        validation_data=validation_proc_dataset,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    return history\n",
    "\n",
    "\n",
    "\n",
    "lr_history = adjust_learning_rate(model=model)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.semilogx(lr_history.history[\"lr\"], lr_history.history[\"loss\"])\n",
    "plt.grid(True)\n",
    "plt.xlabel(\"Learning Rate\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Learning Rate vs Loss\")\n",
    "\n",
    "plt.xlim([1e-7, 1])\n",
    "plt.ylim([0, max(lr_history.history[\"loss\"])])\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.semilogx(lr_history.history[\"lr\"], lr_history.history[\"accuracy\"])\n",
    "plt.grid(True)\n",
    "plt.xlabel(\"Learning Rate\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Learning Rate vs Accuracy\")\n",
    "plt.xlim([1e-7, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498bf653",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "%%timeit -n1 -r1\n",
    "with tf.device('/GPU:0'):\n",
    "    lr = 5e-3\n",
    "    model.compile(\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate = lr),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    history = model.fit(train_proc_dataset, epochs=2, validation_data=validation_proc_dataset)#,callbacks=[EarlyStoppingCallback(patience = 3)])\n",
    "\n",
    "\n",
    "def plot_graphs(history, metric):\n",
    "    plt.plot(history.history[metric])\n",
    "    plt.plot(history.history[f'val_{metric}'])\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(metric)\n",
    "    plt.legend([metric, f'val_{metric}'])\n",
    "    plt.show()\n",
    "    \n",
    "plot_graphs(history=history, metric=\"accuracy\")\n",
    "plot_graphs(history=history, metric=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c10ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(sentence):\n",
    "    \"\"\"\n",
    "    Predict the label for a given sentence\n",
    "\n",
    "    Args:\n",
    "        sentence (str): input sentence to classify\n",
    "        model (tf.keras.Model): trained text classification model\n",
    "\n",
    "    Returns:\n",
    "        str: predicted label for the input sentence\n",
    "    \"\"\"\n",
    "\n",
    "    sentence = tf.expand_dims(sentence, axis=0)\n",
    "    sentence = vectorizer(sentence)\n",
    "    \n",
    "    prediction = model.predict(sentence)\n",
    "    print(prediction)\n",
    "    predicted_label_index = tf.argmax(prediction, axis=1).numpy()[0]\n",
    "    \n",
    "    predicted_label = label_encoder.get_vocabulary()[predicted_label_index]\n",
    "    \n",
    "    return predicted_label, np.max(prediction)*100\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13234e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(model):\n",
    "    with open('model.pkl', 'wb') as f:\n",
    "        pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3069d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def  load_model():\n",
    "    with open('model.pkl', 'rb') as f:\n",
    "        model = pickle.load(f)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60981c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cont = True\n",
    "while cont:\n",
    "    sentence = input(\"Enter a sentence to classify, N to stop :\")\n",
    "    if sentence.capitalize().strip() == \"N\":\n",
    "        cont = False \n",
    "        break\n",
    "    \n",
    "    predicted_label,proba = predict(sentence)\n",
    "    print(f\"The predicted label for the sentence is: {predicted_label} with a probability of {proba:.4f}%\")"
   ]
  }
 ],
 "metadata": {
  "dlai_version": "1.2.0",
  "grader_version": "1",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
